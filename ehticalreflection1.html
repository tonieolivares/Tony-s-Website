<html>
<head> <title> Tony's Portfolio </title>
<style>
  @import url("https://fonts.googleapis.com/css?family=Roboto");
  body {
    line-height: 1.8;
    margin: 0;
    padding: 0;
    color: rgb(110, 120, 125);
    background-image: url("food.png");
    font-size: 17;
  }
  a {
    color: rgb(250, 197, 101);
  }
  a:hover {
    color: rgb(191, 54, 54);
  }
  strong{
    color: rgb(80, 80, 80);
  }
  div {
    margin: 0 auto;
    font-family: 'roboto', sans-serif;
    border-radius: 10px;
    background-color: white;
    width: 70%;
    padding: 7px 20px;
    margin-top: 20px;
    margin-bottom: 10px;
  }
</style>
</head>
<body>
  <div>
    <h1> <strong> Ethical Reflection #1 </strong> </h1>
  </div>
  <div>
    <p> Criminal risk assessments, a highly controversial topic among criminals, courtrooms, and programmers alike. It is controversial due to the fact that the machine seems to have a racial bias against black people, and is inconsistent. </p>
    <p>This is shown in the case of Vernon Prater and Brisha Borden. 41-year old Vernon was caught, having shoplifted $86.35 worth of tools from a Home Depot. He was the seasoned criminal, having already been convicted of armed robbery and attempted armed robbery, spent five years in prison, plus another armed robbery charge. Brisha Borden was caught stealing a small bicycle she found, but when she realized the bike was for a six-year old and way too small, the police had been called.<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">(1)</a> But when the results for the criminal risk assessment came back, Borden had a score of an 8, while Vernon had a score of a 3. What happened? Well, Borden was black and Vernon was white.<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">(1)</a> A few years later, it was discovered that the test had gotten it completely wrong.</p>
    <p>The question of whether or not the test had been in fact racially biased is debated. The questions do not specifically ask what their skin colour is, but they seem to ask questions that are biased against black people, such as whether or not they had both parents growing up, or their neighbourhood, or even where they live. The actual questions seem to be targeting specifically black people, and even if the other person may be more likely to re-offend, the black person is statistically more likely to have a higher rating.<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">(1)</a></p>
    <p>Although, disparities in false positives don’t say anything at all about the disparities in the algorithm itself. Disparate false-positives will always be present every time there are disparate rates of reoffending, whether the subject is black or white, and whether the conclusion was drawn by a computer or the subjective opinion of a judge.<a href="https://www.brookings.edu/blog/up-front/2016/08/22/are-criminal-risk-assessment-scores-racist/" target="_blank">(2)</a></p>
    <p> believe that this algorithm should not be used at all. One study shows that the accuracy is at 80%, which also means there is a 20% FAILURE RATE.<a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" target="_blank">(1)</a> Not only is this less accurate than a polygraph test, but it is also well below the legal standard of ‘guilt beyond a reasonable doubt’<a href="https://en.wikipedia.org/wiki/Reasonable_doubt" target="_blank">(3)</a>  which means it should not have been in court in the first place. Also, let’s assume there were an 100% accurate way of determining this. Then, sentencing would be affected, when it shouldn’t be. The judge should make the sentencing based on what someone ACTUALLY DID, not what they are GOING TO DO IN THE FUTURE. Either way, this should not be allowed to affect the judges sentencing.</p>
    <p>The criminal risk assessment is not ‘racist,’ since it is simply taking factors in that could possibly contribute to a re-offense, and ranks the people. Also, black people make up about 12.3%<a href="https://news.gallup.com/poll/4435/public-overestimates-us-black-hispanic-populations.aspx" target="_blank">(4)</a> of the US population , but they make up about 37.8%<a href="https://www.bop.gov/about/statistics/statistics_inmate_race.jsp" target="_blank">(5)</a> of the prison population.  This disproportionate number is further reason for the algorithm to be the way it is coded.</p>
    <p>In conclusion, not only is the criminal risk assessment not racist, but it unreliable and not a good idea to have in court at all.</p>
 </p>
  </div>
</body>
</html>
